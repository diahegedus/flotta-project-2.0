import streamlit as st
import pandas as pd
import google.generativeai as genai
import json
import os
import io
import time
import sqlite3
import re
import bcrypt
from datetime import datetime

# =========================================================
# 1. BIZTONS√ÅGOS HITEL√âS√çT√âSI RENDSZER (BCRYPT)
# =========================================================
def check_password():
    def password_entered():
        user = st.session_state["username"]
        pwd = st.session_state["password"]
        
        if "users" in st.secrets and user in st.secrets["users"]:
            stored_secret = st.secrets["users"][user]["password"]
            
            is_valid = False
            try:
                if bcrypt.checkpw(pwd.encode('utf-8'), stored_secret.encode('utf-8')):
                    is_valid = True
            except ValueError:
                if pwd == stored_secret:
                    is_valid = True

            if is_valid:
                st.session_state["password_correct"] = True
                st.session_state["logged_in_user"] = user
                st.session_state["role"] = st.secrets["users"][user]["role"]
                del st.session_state["password"]
                del st.session_state["username"]
                return
                
        st.session_state["password_correct"] = False

    if "password_correct" not in st.session_state:
        st.title("üîí Bejelentkez√©s")
        st.text_input("Felhaszn√°l√≥n√©v", on_change=password_entered, key="username")
        st.text_input("Jelsz√≥", type="password", on_change=password_entered, key="password")
        return False
    elif not st.session_state["password_correct"]:
        st.title("üîí Bejelentkez√©s")
        st.text_input("Felhaszn√°l√≥n√©v", on_change=password_entered, key="username")
        st.text_input("Jelsz√≥", type="password", on_change=password_entered, key="password")
        st.error("Hib√°s felhaszn√°l√≥n√©v vagy jelsz√≥.")
        return False
    else:
        return True

if check_password():
    # =========================================================
    # KONFIGUR√ÅCI√ì √âS V√ÅLTOZ√ìK
    # =========================================================
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
        genai.configure(api_key=api_key)
    except KeyError:
        st.error("Kritikus hiba: API kulcs nem tal√°lhat√≥ a secrets.toml f√°jlban.")
        st.stop()

    DB_FILE = "fleet.db"
    
    EXPECTED_FIELDS = [
        "Dokumentum_Tipus", "Alvazszam", "Rendszam", "Vevo_Tulajdonos", 
        "Elado", "Brutto_Vetelar", "Teljesitmeny_kW", "Hengerurtartalom_cm3", "Elso_forgalomba_helyezes"
    ]
    CONF_FIELDS = [f"{f}_Conf" for f in EXPECTED_FIELDS]

    WEIGHTS = {"Alvazszam": 3, "Dokumentum_Tipus": 2, "Brutto_Vetelar": 2, "Rendszam": 1}

    # =========================================================
    # EXCEL SZ√âP√çT≈ê F√úGGV√âNY
    # =========================================================
    def get_formatted_excel(df, sheet_name="Adatok"):
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name=sheet_name)
            worksheet = writer.sheets[sheet_name]
            worksheet.auto_filter.ref = worksheet.dimensions
            from openpyxl.utils import get_column_letter
            for idx, col in enumerate(df.columns, 1):
                col_letter = get_column_letter(idx)
                max_len = max(df[col].astype(str).map(len).max() if not df[col].empty else 0, len(str(col))) + 2
                worksheet.column_dimensions[col_letter].width = min(max_len, 40)
        return output.getvalue()

    # =========================================================
    # SQLITE ADATB√ÅZIS R√âTEG (WAL M√ìD + AUDIT + TRANZAKCI√ìK)
    # =========================================================
    def get_db_connection():
        conn = sqlite3.connect(DB_FILE, timeout=10, check_same_thread=False)
        conn.execute("PRAGMA journal_mode=WAL;")
        return conn

    def init_db():
        conn = get_db_connection()
        cursor = conn.cursor()
        columns = [
            "Alvazszam TEXT PRIMARY KEY", "Rendszam TEXT", "Vevo_Tulajdonos TEXT", 
            "Elado TEXT", "Brutto_Vetelar TEXT", "Teljesitmeny_kW TEXT", 
            "Hengerurtartalom_cm3 TEXT", "Elso_forgalomba_helyezes TEXT",
            "Dokumentum_Tipus TEXT", "Feldolgozasi_Statusz TEXT", 
            "Utolso_Modositas_Ideje TEXT", "Feltolto_User TEXT", 
            "Hiba_Oka TEXT", "Confidence_Score REAL", "Modosito_User TEXT"
        ]
        columns.extend([f"{f} REAL" for f in CONF_FIELDS])
        cursor.execute(f"CREATE TABLE IF NOT EXISTS masterdata ({', '.join(columns)})")
        
        try: cursor.execute("ALTER TABLE masterdata ADD COLUMN Modosito_User TEXT")
        except sqlite3.OperationalError: pass
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS masterdata_audit (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                Alvazszam TEXT,
                Field_Name TEXT,
                Old_Value TEXT,
                New_Value TEXT,
                Modified_By TEXT,
                Timestamp TEXT
            )
        """)
        conn.commit()
        conn.close()

    init_db()

    def load_data():
        conn = get_db_connection()
        df = pd.read_sql_query("SELECT * FROM masterdata", conn)
        conn.close()
        string_cols = ["Hiba_Oka", "Feldolgozasi_Statusz", "Utolso_Modositas_Ideje", "Dokumentum_Tipus", "Modosito_User"]
        for col in string_cols:
            if col in df.columns:
                df[col] = df[col].fillna("").astype(str)
        return df

    def delete_record(alvazszam):
        conn = get_db_connection()
        cursor = conn.cursor()
        try:
            timestamp_now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            current_user = st.session_state["logged_in_user"]
            
            # FIX 2: Atomic Transaction t√∂rl√©sn√©l
            with conn:
                cursor.execute("""
                    INSERT INTO masterdata_audit 
                    (Alvazszam, Field_Name, Old_Value, New_Value, Modified_By, Timestamp) 
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (alvazszam, "RECORD_STATUS", "L√©tez≈ë", "T√ñR√ñLVE", current_user, timestamp_now))
                
                cursor.execute("DELETE FROM masterdata WHERE Alvazszam=?", (alvazszam,))
            return True
        except Exception as e:
            st.error(f"Hiba a t√∂rl√©s sor√°n: {e}")
            return False
        finally:
            conn.close()

    def upsert_record(new_data_dict):
        conn = get_db_connection()
        cursor = conn.cursor()
        
        alvaz = new_data_dict.get("Alvazszam")
        if not alvaz or str(alvaz).lower() in ["null", "none", ""]:
            alvaz = f"ISMERETLEN_{int(time.time())}"
            new_data_dict["Alvazszam"] = alvaz
            
        timestamp_now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        current_user = st.session_state["logged_in_user"]
        
        new_data_dict["Utolso_Modositas_Ideje"] = timestamp_now
        if "Feltolto_User" not in new_data_dict:
            new_data_dict["Feltolto_User"] = current_user

        clean_dict = {k: (str(v) if isinstance(v, (dict, list)) else v) for k, v in new_data_dict.items() if v is not None}
        
        status = "error"
        try:
            # FIX 2: Val√≥di Atomi Tranzakci√≥ (with conn)
            with conn:
                cursor.execute("SELECT * FROM masterdata WHERE Alvazszam=?", (alvaz,))
                existing_row = cursor.fetchone()
                
                # FIX 1: Val√≥s Status logika (new vs updated)
                status = "updated" if existing_row else "new"
                
                if existing_row:
                    col_names_db = [description[0] for description in cursor.description]
                    existing_dict = dict(zip(col_names_db, existing_row))
                    
                    for k, new_v in clean_dict.items():
                        old_v = existing_dict.get(k)
                        if k not in ["Utolso_Modositas_Ideje", "Feltolto_User", "Modosito_User"] and str(old_v) != str(new_v):
                            cursor.execute("""
                                INSERT INTO masterdata_audit 
                                (Alvazszam, Field_Name, Old_Value, New_Value, Modified_By, Timestamp) 
                                VALUES (?, ?, ?, ?, ?, ?)
                            """, (alvaz, k, str(old_v), str(new_v), current_user, timestamp_now))

                cols = list(clean_dict.keys())
                vals = [clean_dict[c] for c in cols]
                placeholders = ", ".join(["?"] * len(cols))
                col_names = ", ".join(cols)
                update_clause = ", ".join([f"{c} = excluded.{c}" for c in cols if c != "Alvazszam"])
                
                sql = f"INSERT INTO masterdata ({col_names}) VALUES ({placeholders}) ON CONFLICT(Alvazszam) DO UPDATE SET {update_clause}"
                cursor.execute(sql, vals)
                
        except Exception as e:
            st.error(f"Adatb√°zis hiba: {e}")
            status = "error"
        finally:
            conn.close()
            
        return status

    # =========================================================
    # VALID√ÅCI√ìS R√âTEG
    # =========================================================
    def validate_ocr_output(data):
        errors = []
        if not data: return False, "Nem valid JSON / AI hiba", 0

        weighted_sum = 0
        total_weight = 0
        for f in EXPECTED_FIELDS:
            w = WEIGHTS.get(f, 1)
            conf_val = data.get(f"{f}_Conf", 0)
            if isinstance(conf_val, (int, float)):
                weighted_sum += (conf_val * w)
                total_weight += w
        
        avg_score = (weighted_sum / total_weight) if total_weight > 0 else 0

        if data.get("low_quality_document") is True:
            errors.append("AI Jelz√©se: Gyenge min≈ës√©g≈±/olvashatatlan k√©p!")
            avg_score -= 15

        doc_type = data.get("Dokumentum_Tipus")
        if not doc_type or str(doc_type).lower() == "null":
            errors.append("Hi√°nyz√≥ Dokumentum T√≠pus"); avg_score -= 20

        alvaz = data.get("Alvazszam")
        if not alvaz or str(alvaz).lower() == "null":
            errors.append("Hi√°nyz√≥ Alv√°zsz√°m"); avg_score -= 40; data["Alvazszam_Conf"] = 0
        else:
            clean_alvaz = str(alvaz).upper().replace(" ", "").replace("-", "")
            VIN_REGEX = r"^[A-HJ-NPR-Z0-9]{17}$"
            if not re.match(VIN_REGEX, clean_alvaz):
                errors.append(f"√ârv√©nytelen VIN form√°tum ({clean_alvaz})")
                avg_score -= 40
                data["Alvazszam_Conf"] = 0

        if str(doc_type).lower() == "sz√°mla":
            vetelar = data.get("Brutto_Vetelar")
            if not vetelar or str(vetelar).lower() in ["null", "none", ""]:
                errors.append("Hi√°nyz√≥ V√©tel√°r (Sz√°mla)"); avg_score -= 20

        low_conf_fields = [f for f in EXPECTED_FIELDS if data.get(f"{f}_Conf", 0) < 80 and str(data.get(f, "")).lower() not in ["null", "none", ""]]
        if low_conf_fields: errors.append(f"Alacsony AI magabiztoss√°g: {', '.join(low_conf_fields)}")

        final_score = max(0, min(100, avg_score))
        if errors: return False, " | ".join(errors), final_score
        return True, "", final_score

    # =========================================================
    # AI KINYER√âS √âS HASH-ALAP√ö CACHE
    # =========================================================
    @st.cache_data(ttl=3600)
    def get_best_models():
        try:
            available = [m.name.replace('models/', '') for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
            preferred = ['gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-1.5-flash-latest', 'gemini-1.5-pro-latest']
            return [m for m in preferred if m in available] or (available[:1] if available else ['gemini-1.5-flash'])
        except Exception:
            return ['gemini-1.5-flash']

    # FIX 4: K√∂lts√©goptimaliz√°lt AI Cache (SHA256 hash a h√°tt√©rben a file_bytes alapj√°n)
    @st.cache_data(show_spinner=False, ttl=86400) # 24 √≥r√°ig eml√©kszik a f√°jlra!
    def extract_with_ai_cached(file_bytes, prompt, models_to_try):
        pdf_part = {"mime_type": "application/pdf", "data": file_bytes}
        last_error = ""
        for model_name in models_to_try:
            try:
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(
                    [prompt, pdf_part],
                    generation_config=genai.GenerationConfig(response_mime_type="application/json")
                )
                
                try: raw_text = response.text
                except Exception as text_e: raise ValueError(f"AI nem adott vissza sz√∂veget: {text_e}")

                # FIX 3: Biztons√°gos JSON extract
                start_idx = raw_text.find('{')
                end_idx = raw_text.rfind('}')
                if start_idx == -1 or end_idx == -1:
                    raise ValueError("No JSON block found in response.")
                
                clean_json_str = raw_text[start_idx : end_idx + 1]
                raw_json = json.loads(clean_json_str)
                
                flat_data = {"low_quality_document": raw_json.get("low_quality_document", False)}
                for field in EXPECTED_FIELDS:
                    if field in raw_json and isinstance(raw_json[field], dict):
                        flat_data[field] = raw_json[field].get("value")
                        if str(flat_data[field]).lower() in ["null", "none", ""]: flat_data[f"{field}_Conf"] = 0
                        else: flat_data[f"{field}_Conf"] = raw_json[field].get("confidence", 0)
                    else:
                        flat_data[field] = raw_json.get(field)
                        flat_data[f"{field}_Conf"] = 0
                        
                return flat_data, ""
            except Exception as e:
                last_error = str(e)
                continue
        return None, f"AI Rendszerhiba: {last_error}"

    def process_document_with_gemini(uploaded_file):
        models_to_try = get_best_models()
        prompt = """
        Elemezd a dokumentumot (forgalmi vagy sz√°mla) √©s add vissza az adatokat szigor√∫an JSON strukt√∫r√°ban!
        Minden mez≈ëh√∂z k√∂telez≈ëen meg kell adnod egy "value" (√©rt√©k) √©s egy "confidence" (0-100) p√°rost.
        EXTRA: Ha a dokumentum nagyon rossz min≈ës√©g≈±, hom√°lyos vagy nehezen olvashat√≥, √°ll√≠tsd be a "low_quality_document": true √©rt√©ket a gy√∂k√©rszinten!
        Kinyerend≈ë mez≈ëk: Dokumentum_Tipus, Alvazszam, Rendszam, Vevo_Tulajdonos, Elado, Brutto_Vetelar, Teljesitmeny_kW, Hengerurtartalom_cm3, Elso_forgalomba_helyezes.
        """
        # H√≠vjuk a CACHE-elt bels≈ë f√ºggv√©nyt a nyers b√°jtokkal!
        return extract_with_ai_cached(uploaded_file.getvalue(), prompt, models_to_try)

    # =========================================================
    # OLDALS√ÅV √âS PIPELINE
    # =========================================================
    with st.sidebar:
        current_user = st.session_state['logged_in_user']
        current_role = st.session_state['role']
        st.write(f"üë§ Felhaszn√°l√≥: **{current_user}**")
        st.write(f"üõ°Ô∏è Szerepk√∂r: *{current_role.capitalize()}*")
        
        if st.sidebar.button("Kijelentkez√©s"):
            for key in ["password_correct", "logged_in_user", "role"]:
                if key in st.session_state: del st.session_state[key]
            st.rerun()

    def run_processing_pipeline(uploaded_files):
        progress_bar = st.progress(0)
        status_placeholder = st.empty()
        new_recs, updated_recs, validation_fails, critical_errors = 0, 0, 0, 0

        for i, file in enumerate(uploaded_files):
            status_placeholder.text(f"St√°tusz: OCR_Feldolgoz√°s_Alatt - {file.name}")
            extracted_data, ai_error_message = process_document_with_gemini(file)
            
            if extracted_data:
                is_valid, error_reason, conf_score = validate_ocr_output(extracted_data)
                extracted_data["Confidence_Score"] = conf_score
                if "low_quality_document" in extracted_data: del extracted_data["low_quality_document"]
                
                if is_valid:
                    extracted_data["Feldolgozasi_Statusz"] = "K√©sz"; extracted_data["Hiba_Oka"] = ""
                else:
                    extracted_data["Feldolgozasi_Statusz"] = "Valid√°ci√≥_Sz√ºks√©ges"; extracted_data["Hiba_Oka"] = error_reason
                    validation_fails += 1
                
                status = upsert_record(extracted_data)
                # FIX 1: Val√≥s status routing a pipeline-ban
                if status == "new": new_recs += 1
                elif status == "updated": updated_recs += 1
            else:
                critical_errors += 1
                upsert_record({"Dokumentum_Tipus": "Ismeretlen", "Feldolgozasi_Statusz": "Hiba", "Hiba_Oka": ai_error_message, "Confidence_Score": 0})
            
            progress_bar.progress((i + 1) / len(uploaded_files))
            if i < len(uploaded_files) - 1: time.sleep(1)

        success_msg = f"Feldolgoz√°s befejezve! √öj bejegyz√©s: {new_recs} | Friss√≠tve: {updated_recs} | Kritikus Hiba: {critical_errors}"
        if validation_fails > 0 or critical_errors > 0: st.warning(f"{success_msg} ‚ö†Ô∏è {validation_fails} dokumentum emberi ellen≈ërz√©st ig√©nyel!")
        else: st.success(success_msg)

    # =========================================================
    # 1. RENDSZER ADMIN N√âZET
    # =========================================================
    if st.session_state["role"] == "admin":
        st.title("üöó IT Rendszer Admin Vez√©rl≈ëpult")
        df_admin = load_data()
        
        st.subheader("üö® AI Megb√≠zhat√≥s√°gi Dashboard")
        if not df_admin.empty:
            df_errors = df_admin[df_admin["Feldolgozasi_Statusz"].isin(["Valid√°ci√≥_Sz√ºks√©ges", "Hiba"])]
            col1, col2, col3, col4 = st.columns(4)
            avg_score = df_admin["Confidence_Score"].mean() if "Confidence_Score" in df_admin.columns else 0
            
            col1.metric("Manu√°lis Ellen≈ërz√©s Kell", len(df_errors))
            col2.metric("√Åtlagos S√∫lyozott Score", f"{avg_score:.1f}%")
            col3.metric("√ñsszes Dokumentum", len(df_admin))
            col4.metric("AI Hib√°k", len(df_admin[df_admin["Feldolgozasi_Statusz"] == "Hiba"]))
            st.markdown("<br>", unsafe_allow_html=True)
            
            tab1, tab2, tab3, tab4, tab5 = st.tabs(["üìå Mez≈ës Szint≈± Analitika", "üìå Hib√°s/Hi√°nyz√≥ Adatok", "üìå Nyers JSON Hiba", "üìà Field Failure Rate", "üïµÔ∏è‚Äç‚ôÇÔ∏è Audit Log"])
            
            with tab1:
                if not df_errors.empty:
                    disp_cols = [c for c in ["Alvazszam", "Alvazszam_Conf", "Rendszam", "Rendszam_Conf", "Brutto_Vetelar", "Brutto_Vetelar_Conf", "Hiba_Oka"] if c in df_errors.columns]
                    st.dataframe(df_errors[disp_cols].sort_values(by="Alvazszam_Conf", ascending=True), use_container_width=True, hide_index=True)
            with tab2:
                df_missing = df_errors[df_errors["Hiba_Oka"].str.contains("Hi√°nyz√≥|√ârv√©nytelen|Min≈ës√©g", na=False, case=False)]
                if not df_missing.empty: st.dataframe(df_missing[["Alvazszam", "Dokumentum_Tipus", "Hiba_Oka", "Confidence_Score"]], use_container_width=True, hide_index=True)
            with tab3:
                df_json = df_errors[df_errors["Feldolgozasi_Statusz"] == "Hiba"]
                if not df_json.empty: st.dataframe(df_json[["Alvazszam", "Feldolgozasi_Statusz", "Hiba_Oka"]], use_container_width=True, hide_index=True)
            with tab4:
                st.markdown("Melyik mez≈ën√©l bizonytalanodik el a legt√∂bbsz√∂r az AI?")
                failure_rates = []
                for f in EXPECTED_FIELDS:
                    if f"{f}_Conf" in df_admin.columns:
                        fail_rate = (df_admin[f"{f}_Conf"] < 80).mean() * 100
                        failure_rates.append({"Mez≈ë": f, "Hiba_Ar√°ny_%": round(fail_rate, 1)})
                if failure_rates:
                    df_failures = pd.DataFrame(failure_rates).sort_values("Hiba_Ar√°ny_%", ascending=False)
                    st.dataframe(df_failures, use_container_width=True, hide_index=True)
            with tab5:
                st.markdown("Ki, mikor, mit √≠rt √°t vagy t√∂r√∂lt a mesteradatban?")
                conn = get_db_connection()
                df_audit = pd.read_sql_query("SELECT * FROM masterdata_audit ORDER BY id DESC", conn)
                conn.close()
                if not df_audit.empty: st.dataframe(df_audit, use_container_width=True, hide_index=True)
                else: st.info("M√©g nem t√∂rt√©nt manu√°lis adatfel√ºl√≠r√°s vagy t√∂rl√©s.")

        st.divider()
        st.subheader("1. K√©zi dokumentum feldolgoz√°s")
        uploaded_files = st.file_uploader("V√°lassza ki a PDF f√°jlokat", type=['pdf'], accept_multiple_files=True)
        if uploaded_files:
            if st.button(f"{len(uploaded_files)} f√°jl feldolgoz√°s√°nak ind√≠t√°sa"): run_processing_pipeline(uploaded_files)

        st.divider()
        with st.expander("üóÑÔ∏è Teljes Master Data"):
            if not df_admin.empty:
                st.dataframe(df_admin, use_container_width=True, hide_index=True)
                excel_data = get_formatted_excel(df_admin, sheet_name='Master_Data')
                st.download_button(label="üì• Master Data let√∂lt√©se (.xlsx)", data=excel_data, file_name='master_data_teljes.xlsx', mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', type="primary")

    # =========================================================
    # 2. √úZLETI ADMINISZTR√ÅTOR N√âZET
    # =========================================================
    elif st.session_state["role"] == "adminisztrator":
        st.title("üöó Flotta Backoffice Vez√©rl≈ëpult")
        df_admin = load_data()
        
        if not df_admin.empty:
            df_pending = df_admin[df_admin["Feldolgozasi_Statusz"].isin(["Valid√°ci√≥_Sz√ºks√©ges", "Hiba"])].copy()
            if not df_pending.empty:
                st.error(f"‚ö†Ô∏è {len(df_pending)} t√©tel manu√°lis jav√≠t√°sra vagy t√∂rl√©sre szorul!")
                st.info("Kattints dupl√°n a cell√°kra a jav√≠t√°shoz, majd pip√°ld be a **J√≥v√°hagy√°s** oszlopot a ment√©shez! Hib√°s felt√∂lt√©s eset√©n haszn√°ld a **T√∂rl√©s** oszlopot.")
                
                cols_to_drop = [c for c in df_pending.columns if c.endswith("_Conf") or c in ["Confidence_Score", "Feltolto_User", "Utolso_Modositas_Ideje", "Modosito_User"]]
                df_editable = df_pending.drop(columns=cols_to_drop, errors='ignore')
                
                df_editable.insert(0, "J√≥v√°hagy√°s", False)
                df_editable.insert(1, "T√∂rl√©s", False)
                
                edited_df = st.data_editor(df_editable, hide_index=True, use_container_width=True, key="data_editor")
                
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("‚úÖ Kijel√∂lt sorok Ment√©se", type="primary"):
                        approved_rows = edited_df[(edited_df["J√≥v√°hagy√°s"] == True) & (edited_df["T√∂rl√©s"] == False)]
                        if not approved_rows.empty:
                            for _, row in approved_rows.iterrows():
                                r_dict = row.to_dict()
                                del r_dict["J√≥v√°hagy√°s"]
                                del r_dict["T√∂rl√©s"]
                                r_dict["Feldolgozasi_Statusz"] = "K√©sz"
                                r_dict["Hiba_Oka"] = "K√©zi jav√≠t√°s"
                                r_dict["Modosito_User"] = current_user
                                upsert_record(r_dict)
                            st.success(f"Sikeresen friss√≠tve {len(approved_rows)} t√©tel!")
                            time.sleep(1)
                            st.rerun()
                        else:
                            st.warning("Nincs kijel√∂lve egyetlen sor sem a j√≥v√°hagy√°shoz.")

                with col2:
                    if st.button("üóëÔ∏è Kijel√∂lt sorok T√∂rl√©se", type="secondary"):
                        to_delete_rows = edited_df[edited_df["T√∂rl√©s"] == True]
                        if not to_delete_rows.empty:
                            for _, row in to_delete_rows.iterrows():
                                delete_record(row["Alvazszam"])
                            st.success(f"Sikeresen t√∂r√∂lve {len(to_delete_rows)} t√©tel az adatb√°zisb√≥l!")
                            time.sleep(1)
                            st.rerun()
                        else:
                            st.warning("Nincs kijel√∂lve egyetlen sor sem a t√∂rl√©shez.")

        st.divider()
        st.subheader("1. K√©zi dokumentum feldolgoz√°s")
        uploaded_files = st.file_uploader("V√°lassza ki a PDF f√°jlokat", type=['pdf'], accept_multiple_files=True, key="bo_uploader")
        if uploaded_files:
            if st.button(f"{len(uploaded_files)} f√°jl feldolgoz√°s√°nak ind√≠t√°sa", key="bo_btn"): run_processing_pipeline(uploaded_files)

        st.divider()
        st.subheader("üìÖ 2. Napi Z√°r√°s √©s Adatk√∂zl≈ë Export")
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        if not df_admin.empty:
            df_daily = df_admin[(df_admin["Utolso_Modositas_Ideje"].str.startswith(today_str)) & (df_admin["Feldolgozasi_Statusz"] == "K√©sz")]
            cols_to_drop = [c for c in df_daily.columns if c.endswith("_Conf") or c == "Confidence_Score"]
            df_daily_clean = df_daily.drop(columns=cols_to_drop, errors='ignore')
            
            if not df_daily_clean.empty:
                st.success(f"Ma feldolgozott, K√âSZ t√©telek sz√°ma: **{len(df_daily_clean)} db**")
                daily_excel = get_formatted_excel(df_daily_clean, sheet_name='Napi_Betoltes')
                st.download_button(label=f"üì• Napi Adatk√∂zl≈ë Let√∂lt√©se (.xlsx)", data=daily_excel, file_name=f'Biztosito_Betoltes_{today_str.replace("-", "")}.xlsx', mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', type="primary")
            else:
                st.info("Ma m√©g nem t√∂rt√©nt sikeres dokumentum-feldolgoz√°s.")
                
        st.divider()
        with st.expander("üóÑÔ∏è Tiszta Master Data"):
            if not df_admin.empty:
                cols_to_drop = [c for c in df_admin.columns if c.endswith("_Conf") or c == "Confidence_Score"]
                df_admin_clean = df_admin.drop(columns=cols_to_drop, errors='ignore')
                st.dataframe(df_admin_clean, use_container_width=True, hide_index=True)
                clean_excel = get_formatted_excel(df_admin_clean, sheet_name='Master_Data')
                st.download_button(label="üì• Tiszta Master Data let√∂lt√©se (.xlsx)", data=clean_excel, file_name='master_data_tiszta.xlsx', mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', type="secondary")

    # =========================================================
    # 3. √úGYF√âL N√âZET
    # =========================================================
    elif st.session_state["role"] == "ugyfel":
        st.title("üìÅ Dokumentum Felt√∂lt≈ë K√∂zpont")
        
        uploaded_files = st.file_uploader("PDF f√°jlok kiv√°laszt√°sa", type=['pdf'], accept_multiple_files=True)
        if uploaded_files:
            if st.button(f"{len(uploaded_files)} f√°jl bek√ºld√©se feldolgoz√°sra", type="primary"): run_processing_pipeline(uploaded_files)

        st.divider()
        st.subheader("Beker√ºlt dokumentumaim √°llapota")
        df_all = load_data()
        if not df_all.empty:
            df_client = df_all[df_all["Feltolto_User"] == current_user]
            if not df_client.empty:
                display_cols = ["Alvazszam", "Dokumentum_Tipus", "Feldolgozasi_Statusz", "Hiba_Oka"]
                st.dataframe(df_client[display_cols], use_container_width=True, hide_index=True)
            else:
                st.info("M√©g nem t√∂lt√∂tt fel dokumentumot.")
